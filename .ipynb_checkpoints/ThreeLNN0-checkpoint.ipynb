{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPLEMENTATION OF 3-LAYER NEURAL NETWORK FROM SCRATCH\n",
    "# BY: M.SOWMIA\n",
    "# ROLL NO: MA19M020\n",
    "\n",
    "This project is about implementing a neural network from scratch and see how it performs on a dataset.By \"scratch\" i mean using only numpy and matplotlib.<br><br>\n",
    "First ,we try to implement a 3-layer NN with the following different options for hyper-parameters\n",
    "- Number of neurons in the Hidden-layer (default=3)\n",
    "- six different types of activation functions\n",
    "    - ELU (exponential linear unit)\n",
    "    - RELU (Rectified Linear Units)\n",
    "    - LeakyReLU (a variant of ReLU)\n",
    "    - Sigmoid (aka the logit fn)\n",
    "    - Tanh (actually a function of sigmoid)\n",
    "    - Softmax (particulary of the output layer when we do multi-class classification)\n",
    "- Learning rate (incase of SD-method)\n",
    "- Different optimisation methods\n",
    "    - Batch-gradient descent\n",
    "    - stochastic gradient descent\n",
    "    - mini-batch gradient descent\n",
    "    \n",
    "## A 3-LAYER NN \n",
    "![title](nn.png)\n",
    "## The number of neurons:\n",
    "The number of neurons in the input layer is the number of features in the training dataset.The number of neurons in the hidden-layer can be chosen.Generally the greater the number of the neurons,the better will be the performance of the algorithm on the training set.The number of neurons in the output layer is chosen according to the number of classes in the label.(2 for a binary classifier and k for a label with k-classes).In each of the layers (except of the output layer)there is a additional neuron which accounts for the bias.Note that this bias term always takes the value 1.\n",
    "## The feed-forward mechanism:\n",
    "The mechanism is very simple.The input layer is basically a input.(A row/instance) of the dataset.<br><br>\n",
    "$$\\Theta^{(1)}=\\text{weights of the first-layer}$$<br>\n",
    "$$\\Theta^{(2)}=\\text{weights of the second-layer}$$<br>\n",
    "For example ,if we had a nn with 5 neurons in input layer,7 in hidden layer(including the bias) and say 2 in output  layer,then our matrix $\\Theta^1$ will be a 7x5 matrix and $\\Theta^2$ will be a 2x7 matrix.<br>\n",
    "As a first-step we calculate<br><br>\n",
    "$$a^{(1)}=X$$<br>\n",
    "$$z^{(2)}=(\\Theta^{(1)})a^{(1)}$$<br>\n",
    "$$a^{(2)}=g(z^{(2)})\\quad\\text{where g is the activation function chosen)}$$<br>\n",
    "$$z^{(3)}=(\\Theta^{(2)})a^2$$ <br>\n",
    "$$a^{(3)}=h(z^{(3)})\\quad\\text{where h is the softmax activation function)}$$<br>\n",
    "Finally we get ,$\\hat y=h(z^{(3)})$ from the output layer,which tells us the which of the k-classes the instance belongs to.\n",
    "## COMPUTING DELTA FOR NEURONS:\n",
    "Once we have predicted the output for a instance using feed-forward mechanism,we can claculate the error by \n",
    "<br><br>\n",
    "$$\\delta^{(3)}=y-\\hat y$$.We use this to find the errors made by the neurons in second layer given by\n",
    "<br><br>\n",
    "$$\\delta^{(2)}=((\\Theta^{(2)})^T\\delta^{(3)}).g'(z^{(2)})$$<br>\n",
    "When the activation function is sigmoid ,we have $$g'(z)=g(z)(1-g(z))$$<br>\n",
    "we can write derivative as\n",
    "$$g'(z^{(2)})=(a^{(2)})^T(1-a^{(2)})$$<br>\n",
    "In general if the NN consists of L layers,then we first claculate<br>\n",
    "$$\\delta^{(L)}=y-\\hat y$$<br>\n",
    "Then for i=L-1,L-2....2,we caclualte error as follows:<br><br>\n",
    "$$\\delta^{(i)}=((\\Theta^{(i)})^T\\delta^{(i+1)}).g'(z^{(i)})$$<br><br>\n",
    "\n",
    "## THE COST-FUNCTION:\n",
    "There are many possible cost functions,however we will try to restrict our selves to the cost fuunction of logistic regression.For neural networks, it is going to be slightly more complicated:<br><br>\n",
    "$$J(\\Theta)=-\\frac{1}{m}\\sum_{i=1}^m\\sum_{j=1}^n\\Big[y_j^{(i)}log(h(x^{(i)})_j+(1-y_j^{(i)})(1-log(h(x^{(i)})_j)\\Big]$$where m is number of training examples and n is number of classes.Now ,let us define<br><br>\n",
    "$$\\Delta_{ij}^{(l)}=\\frac{\\partial J({\\Theta})}{\\partial \\Theta_{ij}^{(l)}}$$<br><br>\n",
    "After claculating $\\delta^{(l)}$ for all layers(except first),we would be having\n",
    "\\begin{equation}\n",
    "\\Delta^{(l)}=\\frac{\\partial J({\\Theta})}{\\partial \\Theta^{(l)}}=\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial J({\\Theta})}{\\partial \\Theta_{10}^{(l)}} & \\frac{\\partial J({\\Theta})}{\\partial \\Theta_{11}^{(l)}} & ....\\\\\n",
    "\\frac{\\partial J({\\Theta})}{\\partial \\Theta_{20}^{(l)}} & \\frac{\\partial J({\\Theta})}{\\partial \\Theta_{21}^{(l)}} & ....\\\\\n",
    "\\vdots\\\\\n",
    "\\vdots\\\\\n",
    "\\frac{\\partial J({\\Theta})}{\\partial \\Theta_{n_{l+1}0}^{(l)}} & \\frac{\\partial J({\\Theta})}{\\partial \\Theta_{n_{l+1}1}^{(l)}} & ....\n",
    "\\end{bmatrix}\n",
    "\\end{equation}<br><br>\n",
    "Where $\\Delta^{(l)}=\\Delta^{(l)}+\\delta^{(l+1)}(a^{(l)})^T$<br>\n",
    "\n",
    "## THE BACK-PROPOGATION ALGORITHM w/o REGULARISATION:\n",
    "The pseudo code for back-prop works as follwos:<br><br>\n",
    "initialise random weights $\\Theta_1,\\Theta_2..\\Theta_L$<br>\n",
    "initiailse $\\Delta_{ij}^{(l)}=0 \\quad\\forall i,j,l$<br><br>\n",
    "while($cost(\\Theta)<\\epsilon)||noiter<threshold)$):<br><br>\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$ for each observation i in the training set:<br><br>\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$ Fwd propogate and compute $h(i)=a_i^{(L)}$<br><br>\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$ Compute $\\delta^{(L)}=\\hat y_i-a_i^{(L)}$<br><br>\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$ Compute $\\delta^{(i)}=((\\Theta^{(i)})^T\\delta^{(i+1)}).g'(z^{(i)})\\quad \\forall i=2,3...L-1$<br><br>\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$ Compute partial derivatives of cost fucntion given by $\\Delta^{(l)} =\\Delta^{(l)}+\\delta^{(l+1)}(a^{(l)})^T \\quad \\forall l=1,2..L-1$<br><br>\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$ Set $\\Delta_{ij}^{(l)}=\\frac{1}{m}[\\Delta_{ij}^{(l)}]\\quad\\forall i,j,l$<br><br>\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$ Feed the partial derivatives $\\Delta_{ij}^{(l)}$ to gradient descent (or any optimization algorithm) and get the updated weights<br><br>\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$ Update $\\Theta_1,\\Theta_2..\\Theta_L$<br><br>\n",
    "\n",
    "## THE BACK-PROPOGATION ALGORITHM with REGULARISATION:\n",
    "The pseudo code for back-prop works as follwos:<br><br>\n",
    "initialise random weights $\\Theta_1,\\Theta_2..\\Theta_L$<br>\n",
    "initiailse $\\Delta_{ij}^{(l)}=0 \\quad\\forall i,j,l$<br><br>\n",
    "while($cost(\\Theta)<\\epsilon)||noiter<threshold)$):<br><br>\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$ for each observation i in the training set:<br><br>\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$ Fwd propogate and compute $h(i)=a_i^{(L)}$<br><br>\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$ Compute $\\delta^{(L)}=\\hat y_i-a_i^{(L)}$<br><br>\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$ Compute $\\delta^{(i)}=((\\Theta^{(i)})^T\\delta^{(i+1)}).g'(z^{(i)})\\quad \\forall i=2,3...L-1$<br><br>\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$ Compute partial derivatives of cost fucntion given by $\\Delta^{(l)} =\\Delta^{(l)}+\\delta^{(l+1)}(a^{(l)})^T \\quad \\forall l=1,2..L-1$<br><br>\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$Set $\\Delta_{ij}^{(l)}=\\frac{1}{m}[\\Delta_{ij}^{(l)}]+\\lambda\\Theta_{ij}^{(l)}\\quad\\forall i,j\\neq 0,l$  and $\\Delta_{i0}^{(l)}=\\frac{1}{m}[\\Delta_{i0}^{(l)}]\\quad\\forall i,l$<br><br>\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$ Feed the partial derivatives $\\Delta_{ij}^{(l)}$ to gradient descent (or any optimization algorithm) and get the updated weights<br><br>\n",
    "$\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$ Update $\\Theta_1,\\Theta_2..\\Theta_L$<br><br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing datasets \n",
    "Bleow we generated a set of points from dataset make_moons.It has two classes as denoted by pink and violet dots.Our aim is to find a non-linear decision boundary that fits the dataset and preferably works good for non-seen dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all required datasets\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.datasets\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeLayerNN:\n",
    "    \n",
    "    #class objects initilization\n",
    "    #NN_config =[a,b,c] a-#no Neurons in input layer,b=# of neurons in hidden layer, c=#of neurons in output layer\n",
    "    #Theta1~bxa,Theta2~cxb,b1~bx1,b2~cx1\n",
    "    \n",
    "    def __init__(self,NN_config,Theta1,Theta2,b1,b2,optimization_method=\"BGD\",lr=0.1,reg_param=0,cost_func=\"log-loss\",epsilon=0.01,noiter=10,activation=\"sigmoid\",alpha=0.01):\n",
    "        \n",
    "        #intilising the configuartion of NN\n",
    "        self.input_layer=NN_config[0]\n",
    "        self.hidden_layer=NN_config[1]\n",
    "        self.output_layer=NN_config[2]\n",
    "        \n",
    "        #initilising weights and bias\n",
    "        self.Theta1=Theta1\n",
    "        self.Theta2=Theta2\n",
    "        self.b1=b1\n",
    "        self.b2=b2\n",
    "        self.unrolled_weights=np.concatenate((np.ravel(self.Theta1),np.ravel(self.Theta2)),axis=0)\n",
    "        \n",
    "        #intialising deltas for weights and bias\n",
    "        self.delta_Theta1 = np.zeros(self.Theta1.shape)\n",
    "        self.delta_Theta2 = np.zeros(self.Theta2.shape)\n",
    "        self.delta_b1 = np.zeros(self.b1.shape)\n",
    "        self.delta_b2 = np.zeros(self.b2.shape)\n",
    "         \n",
    "        #initilising optimization algorithm (SGD,BGD,MBGD)\n",
    "        self.optimization_method=optimization_method\n",
    "        #intialising learning rate(SGD,BGD,MBGD)\n",
    "        self.lr=lr\n",
    "        #initilaising regularisation parameter\n",
    "        self.reg_param=reg_param\n",
    "        #intialising the type of cost function(\"log-loss\",\"MSE\")\n",
    "        self.cost_func=cost_func\n",
    "        #intilising the max number of iterations and threshold (whihever occurs first)to stop optimization algortihms\n",
    "        self.noiter=noiter\n",
    "        self.epsilon=epsilon\n",
    "        #intilising the activation of hidden-layer(\"tanh\",\"sigmoid\",\"RELU\",\"LeakyRELU\",\"ELU\",\"Softmax\")\n",
    "        self.activation=activation\n",
    "        #initialsing alpha(used in leaky RELU)\n",
    "        self.alpha=alpha\n",
    "        \n",
    "    #Defining a function to calculate the cost_function\n",
    "    def cost(self,y_actual,y_pred,Theta=None):\n",
    "        if Theta is None:\n",
    "            Theta = self.unrolled_weights\n",
    "        if(self.cost_func==\"log-loss\"):\n",
    "            pad_term=10e-5 #adding this terms to avoid accidently calculating log(0)\n",
    "            m=len(y_actual)\n",
    "            return ((-1/m*(np.sum(y_actual*np.log(y_pred.clip(pad_term)))))+(self.reg_param/(2*m))*(np.sum(Theta**2)))\n",
    "        elif(self.cost_func==\"MSE\"):\n",
    "            loss = (1/2)*(np.square(y-y_hat).mean(axis = 0))\n",
    "            return (np.sum(loss, axis = 0)+(self.reg_param/(2*m))*(np.sum(Theta**2)))\n",
    "        \n",
    "    \n",
    "    #defining a funciton to calculate the derivative of the cost function\n",
    "    #NOTE:we generrally use back-prop to calculate derivatives,howver for gradient checking ,we might require this function\n",
    "    #Please use this function ony for cross-checking,don't put it in the loop,it will take centuries to run!!\n",
    "    def gradient_checking(self,y_actual,y_pred):\n",
    "        epsilon=0.01\n",
    "        self.unrolled_weights=np.concatenate((np.ravel(self.Theta1),np.ravel(self.Theta2)),axis=0)\n",
    "        derivatives=[]\n",
    "        for i in range(len(self.unrolled_weights)):\n",
    "            fwd_diff=self.unrolled_weights\n",
    "            fwd_diff[i]=fwd_diff[i]+epsilon\n",
    "            bwd_diff=self.unrolled_weights\n",
    "            bwd_diff[i]=bwd_diff[i]-epsilon\n",
    "            derivatives.append((self.cost(y_actual,y_pred,fwd_diff)-self.cost(y_actual,y_pred,bwd_diff))/(2*epsilon))\n",
    "        return derivatives\n",
    "    \n",
    "    \n",
    "    #defining a function to give activations for the hidden layer..Z1=Theta1xX+b1 and Z2=Theta2xa2+b2\n",
    "    def activation_func(self,z):\n",
    "        if(self.activation==\"sigmoid\"):\n",
    "            return 1/(1+np.exp(-z))\n",
    "        elif(self.activation==\"tanh\"):\n",
    "            return ((2/(1+np.exp(-2*z)))-1)\n",
    "        elif(self.activation==\"RELU\"):\n",
    "            return z.clip(min=0)\n",
    "        elif(self.activation==\"LeakyRELU\"):\n",
    "            return (self.alpha*z.clip(max=0))+z.clip(min=0)\n",
    "        elif(self.activation==\"ELU\"):\n",
    "            return ((self.alpha*np.exp(z.clip(max=0)-1))+z.clip(min=0))\n",
    "        \n",
    "    #defining function for derivative of activation functions\n",
    "    #NOTE:In below code a=g(z) ie derivatives are defined interms of function values for all activations \n",
    "    def activation_derivative(self,a):\n",
    "        if(self.activation==\"sigmoid\"):\n",
    "            return np.multiply(a, (1.0 - a))\n",
    "        elif(self.activation==\"tanh\"):\n",
    "            return 1-(a**2)\n",
    "        elif(self.activation==\"RELU\"):\n",
    "            return (np.sign(a)+1)/2\n",
    "        elif(self.activation==\"LeakyRELU\"):\n",
    "            return np.sign(a).clip(self.alpha)\n",
    "        elif(self.activation==\"ELU\"):\n",
    "            return ((self.alpha*np.exp(a.clip(max=0)))-(self.alpha*np.sign(a.clip(min=0)))+a.clip(min=0))\n",
    "        \n",
    "        \n",
    "    #defining the soft-max function for the output layer h(x)\n",
    "    #NOTE:for the below function z=Theta2xa2+b2\n",
    "    def softmax(self,z):\n",
    "        scores=np.exp(z)\n",
    "        total_score=np.sum(np.exp(z))\n",
    "        return scores/total_score\n",
    "    \n",
    "    \n",
    "    #Defining functions for FORWARD PROPOGATION\n",
    "    def forward_propogate(self,x):\n",
    "        self.a0 = x.copy()\n",
    "        self.z1 = self.Theta1.dot(self.a0) + self.b1\n",
    "        self.a1 = self.activation_func(self.z1)\n",
    "        self.z2 = self.Theta2.dot(self.a1) + self.b2\n",
    "        self.a2 = self.softmax(self.z2)\n",
    "        return self.a2\n",
    "    \n",
    "    #Defining function for BACKWARD PROPOGATION(to claculate the derivative of cost function with respect to weights)\n",
    "    def backward_propogate(self,y,y_hat):\n",
    "        #calculate delta3,note:y_hat is the h(x) value\n",
    "        y_actual=np.zeroes(len(y))\n",
    "        y_actual[y]=1\n",
    "        self.delta3 = np.multiply(-y_actual+y_hat,self.activation_derivative(self.a2))\n",
    "        #therefore calculate delta2 \n",
    "        self.delta2 = np.multiply(self.Theta2.T.dot(self.delta3),self.activation_derivative(self.a1))\n",
    "        #We now calculate derivatives wrt the weights and bias of the cost \n",
    "        self.delta_Theta1 = np.outer(self.delta2,self.a0)\n",
    "        self.delta_Theta2 = np.outer(slef.delta3,self.a1)\n",
    "        self.delta_b1 = self.delta2\n",
    "        self.delta_b2 = self.delta3\n",
    "        return self.delta_Theta1,self.delta_Theta2,self.delta_b1,self.delta_b2\n",
    "        \n",
    "        \n",
    "    #functions for predicting ,GD\n",
    "    def predict(self,x):\n",
    "        y_pred=self.forward_propogate(x)\n",
    "        return np.argmax(y_pred)\n",
    "    \n",
    "    #Defining function for plotting the decision boundary,please use this function only once at the last,it is very slow\n",
    "    def plot_decision_boundary(self,X, y):\n",
    "        # Set min and max values and give it some padding\n",
    "        x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "        y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "        h = 0.1\n",
    "        # Generate a grid of points with distance h between them\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "        # Predict the function value for the whole gid\n",
    "        grid_pts=np.c_[xx.ravel(), yy.ravel()]\n",
    "        Z=[]\n",
    "        for x in grid_pts:\n",
    "            Z.append(self.predict(x))\n",
    "        Z=np.array(Z)\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        # Plot the contour and training examples\n",
    "        plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
    "        plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral)\n",
    "        \n",
    "\n",
    "    \n",
    "        \n",
    "    def Train_Test_compare(self,x_train,y_train,x_test,y_test):\n",
    "        #I am just implementing the delta rule as discussed in the coursera Andrew Ng course\n",
    "        #take cases for BGD,MBGD,SGD and accordingly\n",
    "        if(self.optimization_method==\"BGD\"):\n",
    "            self.train_error=[]\n",
    "            self.test_error=[]\n",
    "            epochs=0\n",
    "            while(epochs<self.noiter):\n",
    "                epochs+=1\n",
    "                #calculating the test MSE\n",
    "                y_test_pred=[]\n",
    "                y_train_pred=[]\n",
    "                del_Theta1=np.zeros(self.delta_Theta1.shape)\n",
    "                del_Theta2=np.zeros(self.delta_Theta2.shape)\n",
    "                del_b1=np.zeros(self.delta_b1.shape)\n",
    "                del_b2=np.zeros(self.delta_b2.shape)\n",
    "                for i in range(len(x_test)):\n",
    "                    y_test_pred.append(self.forward_propogate(x_test[i]))\n",
    "                y_test_pred=np.array(y_test_pred)\n",
    "                self.test_error.append(self.cost(y_test,y_test_pred))\n",
    "                #training with full batch\n",
    "                for i in range(len(x_train)):\n",
    "                    y_pred=self.forward_propogate(x_train[i])\n",
    "                    y_train_pred.append(self.forward_propogate(y_pred))\n",
    "                    #Finding all deltas and storing them\n",
    "                    a,b,c,d=self.backward_propogate(y_train,y_pred)\n",
    "                    del_Theta1 += a\n",
    "                    del_Theta2 + b\n",
    "                    del_b1 += c\n",
    "                    del_b2 += d\n",
    "                #finding average of deltas\n",
    "                self.delta_Theta1 = ((del_Theta1)/len(x_train))+self.reg_param*self.Theta1 \n",
    "                self.delta_Theta2 = (del_Theta2/len(x_train))+self.reg_param*self.Theta2\n",
    "                self.delta_b1 = (del_b1)/len(x_train)\n",
    "                self.delta_b2 = (del_b2)/len(x_train)\n",
    "                #updating weights\n",
    "                self.Theta1 -= self.lr*self.delta_Theta1\n",
    "                self.Theta2 -= self.lr*self.delta_Theta2\n",
    "                self.b1 -= self.lr*self.delta_b1\n",
    "                self.b2 -= self.lr*self.delta_b2\n",
    "                self.unrolled_weights = np.concatenate((np.ravel(self.Theta1),np.ravel(self.Theta2)),axis=0)\n",
    "                self.train_error.append(self.cost(y_train,y_train_pred))\n",
    "                #If no significant change in test MSE  ...we stop\n",
    "                if(epochs>5):\n",
    "                    if(np.abs(self.test_error(epochs-1)-self.test_error(epochs-2))<self.epsilon):\n",
    "                        print(\"The number of iterations required for convergence is:\", epochs)\n",
    "                        break\n",
    "            #Now plot the decision boundary with optimum values of weights and bias found\n",
    "            self.plot_decision_boundary(x_train,y_train)\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "            \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x19d51361438>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Creating my own dataset to check the working of the algorithm\n",
    "np.random.seed(0)\n",
    "X, y = sklearn.datasets.make_moons(20, noise=0.20)\n",
    "plt.scatter(X[:,0], X[:,1], s=40, c=y, cmap=plt.cm.Spectral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_config=[2,3,2]\n",
    "np.random.seed(10)\n",
    "W1 = np.random.rand(3,2)\n",
    "W2 = np.random.rand(2,3)\n",
    "b1 = np.random.rand(3,1)\n",
    "b2 = np.random.rand(2,1)\n",
    "model=ThreeLayerNN(NN_config,W1,W2,b1,b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (10,) (10,2,3) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-761ef2de9d46>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrain_Test_compare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-28-ba5494d4a7bb>\u001b[0m in \u001b[0;36mTrain_Test_compare\u001b[1;34m(self, x_train, y_train, x_test, y_test)\u001b[0m\n\u001b[0;32m    177\u001b[0m                     \u001b[0my_test_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_propogate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m                 \u001b[0my_test_pred\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_error\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m                 \u001b[1;31m#training with full batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-28-ba5494d4a7bb>\u001b[0m in \u001b[0;36mcost\u001b[1;34m(self, y_actual, y_pred, Theta)\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[0mpad_term\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10e-5\u001b[0m \u001b[1;31m#adding this terms to avoid accidently calculating log(0)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[0mm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_actual\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_actual\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpad_term\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreg_param\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTheta\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m         \u001b[1;32melif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcost_func\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m\"MSE\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0my_hat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (10,) (10,2,3) "
     ]
    }
   ],
   "source": [
    "model.Train_Test_compare(X[:10,:],y[:10],X[10:,:],y[10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
